{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjq/7pPCou3k1b6Dy4NcFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albordunos/topic_modeling/blob/main/LDA_TG_CSV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install pandas xlrd nltk gensim pyLDAvis chardet matplotlib wordcloud razdel pymorphy2 stopwords\n",
        "\n",
        "# Импорт необходимых библиотек\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer  # Импортируем лемматизатор\n",
        "import re\n",
        "import string\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "import chardet\n",
        "from gensim.models import Phrases  # Импортируем класс Phrases\n",
        "from gensim.models.coherencemodel import CoherenceModel  # Импортируем CoherenceModel\n",
        "import matplotlib.pyplot as plt  # Для визуализации\n",
        "from wordcloud import WordCloud\n",
        "from razdel import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import pymorphy2\n",
        "import stopwords  # Импортируем библиотеку stopwords\n",
        "\n",
        "# Загрузка ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "B3OljxR5KKO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. Загрузка данных ===\n",
        "# Укажите путь к файлу .csv\n",
        "file_path = '/content/MID1.csv'\n",
        "\n",
        "# Определение кодировки файла\n",
        "with open(file_path, 'rb') as f:\n",
        "    result = chardet.detect(f.read(10000))\n",
        "    detected_encoding = result['encoding']\n",
        "    print(f\"Определённая кодировка: {detected_encoding}\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "lRIBlDh-LM5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение CSV-файла с использованием определенной кодировки и обработкой некорректных строк\n",
        "df = pd.read_csv(file_path, encoding=detected_encoding, sep=';', on_bad_lines='skip')\n",
        "\n",
        "# Вывод первых строк для проверки\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "LVOcicjdME1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si4YgjDsJ6WR"
      },
      "outputs": [],
      "source": [
        "# Укажите название столбца с текстом\n",
        "text_column = 'message'  # Замените на реальное название столбца\n",
        "\n",
        "# Вывод информации о количестве строк и слов в исходных данных\n",
        "num_rows = df.shape[0]\n",
        "num_words = df[text_column].str.split().str.len().sum()  # Считаем общее количество слов в указанном столбце\n",
        "\n",
        "print(f'Количество строк в исходных данных: {num_rows}')\n",
        "print(f'Количество слов в исходных данных: {num_words}')\n",
        "\n",
        " # === 2. Предобработка текста ===\n",
        "# Определение своих стоп-слов\n",
        "custom_stop_words = ['мид', 'россия', 'россии', 'который', 'которые', 'которых', 'также', 'именно', 'включая', 'сегодня', 'лавров', 'захарова', 'минимтр', 'иностранных', 'дел', 'год', 'российский', 'федерация']\n",
        "\n",
        "# Получаем стоп-слова для русского языка из библиотеки stopwords\n",
        "russian_stop_words = stopwords.get_stopwords('ru')\n",
        "\n",
        "# Объединяем с пользовательскими стоп-словами\n",
        "stop_words = set(russian_stop_words + custom_stop_words)\n",
        "\n",
        "# Инициализация лемматизатора\n",
        "morph = pymorphy2.MorphAnalyzer()  # Инициализация лемматизатора pymorphy2\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Проверка на NaN\n",
        "        return []\n",
        "\n",
        "    # Приведение текста к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Удаление URL\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Удаляем URL\n",
        "\n",
        "    # Удаление пунктуации и чисел, а также лишних пробелов\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Заменяем пунктуацию на пробел\n",
        "    text = re.sub(r'\\d+', '', text)  # Удаляем числа\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Заменяем несколько пробелов на один\n",
        "    text = text.strip()  # Удаление пробелов в начале и конце строки\n",
        "\n",
        "    # Токенизация текста\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Лемматизация токенов и фильтрация: удаление неалфавитных и стоп-слов\n",
        "    filtered_tokens = [\n",
        "        morph.parse(token.text)[0].normal_form\n",
        "        for token in tokenize(text)\n",
        "        if token.text.isalnum() and token.text not in stop_words and len(token.text) > 2\n",
        "    ]\n",
        "\n",
        "    return filtered_tokens  # Возвращаем список токенов\n",
        "\n",
        "\n",
        "# Применение функции очистки текста к указанному столбцу\n",
        "df['cleaned_text'] = df[text_column].astype(str).apply(preprocess_text)\n",
        "\n",
        "# === 3. Создание n-грамм ===\n",
        "# Создаем модель биграмм\n",
        "bigram_model = Phrases(df['cleaned_text'], min_count=5, threshold=10)\n",
        "\n",
        "# Применяем биграммную модель к очищенным токенам\n",
        "df['bigrams'] = df['cleaned_text'].apply(lambda tokens: bigram_model[tokens])\n",
        "\n",
        "# === 4. Создание корпуса и словаря ===\n",
        "# Преобразуем очищенные тексты в список токенов\n",
        "tokenized_texts = df['cleaned_text'].tolist()  # Получаем список списков токенов\n",
        "tokenized_bigrams = df['bigrams'].tolist()  # Получаем список биграмм\n",
        "\n",
        "# Создаем словарь (gensim Dictionary)\n",
        "dictionary = corpora.Dictionary(tokenized_texts + tokenized_bigrams)\n",
        "\n",
        "# Удаление редких и слишком частых слов (по желанию)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.7)\n",
        "\n",
        "# Создаем корпус (список мешков слов)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_texts + tokenized_bigrams]\n",
        "\n",
        "# === 5. TF-IDF модель ===\n",
        "# Применяем TF-IDF модель к корпусу\n",
        "tfidf = TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "# Вывод статистики и первых строк для проверки\n",
        "num_docs = len(tokenized_texts)\n",
        "num_words = sum([len(tokens) for tokens in tokenized_texts])\n",
        "print(f\"Количество документов (строк): {num_docs}\")\n",
        "print(f\"Общее количество слов: {num_words}\")\n",
        "print(df[['message', 'cleaned_text', 'bigrams']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#====6 обучение модели====\n",
        "# Параметры для подбора количества тем\n",
        "num_topics = int(input(\"Введите количество тем: \"))\n",
        "\n",
        "# Создание модели LDA с заданным количеством тем\n",
        "lda_model = LdaModel(corpus=corpus,\n",
        "                     id2word=dictionary,\n",
        "                     num_topics=num_topics,\n",
        "                     passes=5)\n",
        "\n",
        "# Функция для получения доминирующей темы для документа\n",
        "def get_dominant_topic(lda_model, doc_topics):\n",
        "    return max(doc_topics, key=lambda item: item[1])\n",
        "\n",
        "# Получение тем для всех документов\n",
        "doc_topics = lda_model.get_document_topics(corpus)\n",
        "\n",
        "# Вывод результатов\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Тема: {}'.format(idx))\n",
        "    print('Слова: {}'.format(topic))\n",
        "\n",
        "    # Получение документов, относящихся к теме\n",
        "    doc_topics = lda_model.get_document_topics(corpus)\n",
        "    docs = [doc_id for doc_id, topic_values in enumerate(doc_topics) if max(topic_values, key=lambda x: x[1])[0] == idx]\n",
        "\n",
        "    # Вывод номеров строк для темы\n",
        "    print('Документы (строки):', docs)\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "QC--RbDpBML4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7.1 Визуализация тем с помощью pyLDAvis ===\n",
        "# Подготовка данных для визуализации\n",
        "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(lda_vis)\n",
        "\n",
        "# --- 7.2. Распределение тем (pyLDAvis) ---\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "id": "pEQZZjIwB9zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 8. Получение наиболее вероятных слов для каждой темы ===\n",
        "topics_words = lda_model.show_topics(formatted=False)\n",
        "\n",
        "# === 9. Облака слов для каждой темы ===\n",
        "# Создание облака слов для каждой темы\n",
        "for topic_id, topic in topics_words:\n",
        "    word_freq = {word: freq for word, freq in topic}\n",
        "\n",
        "    # Генерация облака слов\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "    # Визуализация облака слов\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Облако слов для темы {topic_id}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "isNW3jQpCD84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=== 10. Сохранение результатов ===\n",
        "# Сохранение модели и словаря (по желанию)\n",
        "lda_model.save('/content/lda_model.gensim')\n",
        "dictionary.save('/content/dictionary.gensim')\n",
        "\n",
        "# Сохранение очищенного текста\n",
        "df.to_csv('/content/cleaned_texts.csv', index=False)\n",
        "print(\"Результаты сохранены в файл: /content/cleaned_texts.csv\")"
      ],
      "metadata": {
        "id": "5G7kHSF4uHx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl  # Для сохранения в .xlsx\n",
        "\n",
        "\n",
        "# Функция для получения доминирующей темы для документа\n",
        "def get_dominant_topic(doc_topics):\n",
        "    return max(doc_topics, key=lambda item: item[1])\n",
        "\n",
        "# Получение тем для всех документов\n",
        "doc_topics = lda_model.get_document_topics(corpus)\n",
        "\n",
        "# Список для хранения данных\n",
        "data = []\n",
        "\n",
        "# Вывод результатов и сбор данных\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Тема: {}'.format(idx))\n",
        "    print('Слова: {}'.format(topic))\n",
        "\n",
        "    # Получение документов, относящихся к теме\n",
        "    docs = [doc_id for doc_id, topic_values in enumerate(doc_topics) if get_dominant_topic(topic_values)[0] == idx]\n",
        "\n",
        "    # Добавляем информацию о теме и связанных документах в список\n",
        "    for doc_id in docs:\n",
        "        data.append({'Topic ID': idx, 'Words': topic, 'Document ID': doc_id})\n",
        "\n",
        "    # Вывод номеров строк для темы\n",
        "    print('Документы (строки):', docs)\n",
        "    print()\n",
        "\n",
        "# Создание DataFrame из собранных данных\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Сохранение DataFrame в Excel файл\n",
        "df.to_excel('/content/MID1.xlsx', index=False)  # Для .xlsx\n",
        "\n",
        "print(\"Данные успешно сохранены в /content/MID1.xlsx\")\n"
      ],
      "metadata": {
        "id": "EvVlU5XfYqXo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}